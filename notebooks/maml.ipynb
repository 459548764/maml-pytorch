{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "https://hacks.deeplearning.jp/pytorch%E3%81%AEdataloader/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "n_shot = 1\n",
    "n_class = 10\n",
    "n_local_update = 5\n",
    "batch_size = n_class\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# TODO ↓消す\n",
    "def batchnorm(input, weight=None, bias=None, running_mean=None, running_var=None, training=True, eps=1e-5, momentum=0.1):\n",
    "    ''' momentum = 1 restricts stats to the current mini-batch '''\n",
    "    # This hack only works when momentum is 1 and avoids needing to track running stats\n",
    "    # by substuting dummy variables\n",
    "    running_mean = torch.zeros(np.prod(np.array(input.data.size()[1]))).cuda()\n",
    "    running_var = torch.ones(np.prod(np.array(input.data.size()[1]))).cuda()\n",
    "    return F.batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\n",
    "\n",
    "\n",
    "class OmniglotNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        self.h=64\n",
    "        super(OmniglotNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, self.h, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(self.h, self.h, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(self.h, self.h, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(self.h, momentum=1, affine=True)\n",
    "        self.bn2 = nn.BatchNorm2d(self.h, momentum=1, affine=True)\n",
    "        self.bn3 = nn.BatchNorm2d(self.h, momentum=1, affine=True)\n",
    "        self.fc = nn.Linear(self.h, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))), 2)\n",
    "        x = x.view(x.size(0), self.h)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    # for MAML local optimize\n",
    "    def manual_forward(self, x, params):\n",
    "        \n",
    "        x = F.conv2d(x, params['conv1.weight'].to(device), params['conv1.bias'].to(device))\n",
    "        x = batchnorm(x, weight = params['bn1.weight'], bias = params['bn1.bias'], momentum=1)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.conv2d(x, params['conv2.weight'].to(device), params['conv2.bias'].to(device))\n",
    "        x = batchnorm(x, weight = params['bn2.weight'], bias = params['bn2.bias'], momentum=1)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.conv2d(x, params['conv3.weight'].to(device), params['conv3.bias'].to(device))\n",
    "        x = batchnorm(x, weight = params['bn3.weight'], bias = params['bn3.bias'], momentum=1)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(x.size(0), self.h)\n",
    "        x = F.linear(x, params['fc.weight'].to(device), params['fc.bias'].to(device))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# class OmniglotNet(nn.Module):\n",
    "#     def __init__(self, n_class):\n",
    "#         super(OmniglotNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, n_class)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = F.log_softmax(self.fc2(x), dim=1)\n",
    "#         return x\n",
    "\n",
    "#     # for MAML local optimize\n",
    "#     def manual_forward(self, x, params):\n",
    "#         x = F.relu(F.max_pool2d(\n",
    "#             F.conv2d(x, params['conv1.weight'].to(device), params['conv1.bias'].to(device)), 2))\n",
    "#         x = F.relu(F.max_pool2d(\n",
    "#             F.conv2d(x, params['conv2.weight'].to(device), params['conv2.bias'].to(device)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(\n",
    "#             F.linear(x, params['fc1.weight'].to(device), params['fc1.bias'].to(device)))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = F.log_softmax(\n",
    "#             F.linear(x, params['fc2.weight'].to(device), params['fc2.bias'].to(device)), dim=1)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "def train(model, device, train_data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    for data, target in train_data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        train_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_data_loader.dataset)\n",
    "    train_acc /= len(train_data_loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, device, test_data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            test_loss += loss\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            test_acc += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_data_loader.dataset)\n",
    "    test_acc /= len(test_data_loader.dataset)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotOriginDataset(Dataset):\n",
    "    def __init__(self, path_to_lang, n_class, train, train_index, transform):\n",
    "\n",
    "        self.data = []\n",
    "        self.path = path_to_lang\n",
    "        \n",
    "        labels = sorted(os.listdir(path_to_lang))[:n_class]\n",
    "        \n",
    "        for label_i, label in enumerate(labels):\n",
    "            path_to_label = os.path.join(path_to_lang, label)\n",
    "            chars = np.array(sorted(os.listdir(path_to_label)))\n",
    "            if train:\n",
    "                chars = chars[train_index]\n",
    "            else:\n",
    "                test_indices = list(set(np.arange(20)) - set(train_index)) # 各文字20枚ずつ入ってる\n",
    "                chars = chars[test_indices]\n",
    "            for char in chars:\n",
    "                path_to_char = os.path.join(path_to_label, char)\n",
    "                image = io.imread(path_to_char)\n",
    "                label_i = np.array(label_i)\n",
    "                self.data.append([image, label_i])\n",
    "            \n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transform(self.data[idx])\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[0], sample[1]\n",
    "        image = image / 255\n",
    "        image = image.reshape([28,28, 1])\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = np.array(image, np.float32)\n",
    "\n",
    "        return [torch.from_numpy(image), torch.from_numpy(label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# chars = []\n",
    "# path_to_langs = \"../data/omniglot_mini/images_background/\"\n",
    "\n",
    "# for path_to_lang in os.listdir(path_to_langs):\n",
    "#     path_to_chars = os.path.join(path_to_langs, path_to_lang)\n",
    "#     for path_to_char in os.listdir(path_to_chars):\n",
    "#         chars.append(os.path.join(path_to_chars, path_to_char))\n",
    "# print(len(chars))\n",
    "# chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# train_indices = np.random.randint(20, size=10)\n",
    "# train_indices\n",
    "# class OmniglotAugmentedDataset(Dataset):\n",
    "#     def __init__(self, path_to_chars, train, train_indices, transform):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/omniglot_mini/images_background/Japanese_(hiragana)/\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: tensor([9, 7, 6, 1, 3, 0, 5, 4, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "local_task_train_data_loader = DataLoader(\n",
    "    OmniglotOriginDataset(\"../data/omniglot_mini/images_background/Japanese_(hiragana)/\", \n",
    "                    n_class=n_class,\n",
    "                    train=True,\n",
    "                    train_index=[0],\n",
    "                    transform=transforms.Compose([\n",
    "                        ToTensor(),\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(local_task_train_data_loader.dataset.path)\n",
    "\n",
    "for data, target in local_task_train_data_loader: # only have one batch\n",
    "    plt.figure(figsize=(10,1))\n",
    "    for i, x in enumerate(data):\n",
    "        plt.subplot(1, batch_size, i+1); plt.imshow(x[0])\n",
    "    plt.show()\n",
    "    print(\"y_true:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit\n",
    "- task=Japanese-hiragana, 20classes, 1shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 45.97it/s, epoch=10, train_loss=0.0671, train_acc=1, test_loss=2.04, test_acc=0.289]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: tensor([5, 4, 1, 9, 0, 6, 3, 2, 8, 7])\n",
      "y_true: tensor([5, 4, 1, 9, 0, 6, 3, 2, 8, 7])\n"
     ]
    }
   ],
   "source": [
    "local_task_train_data_loader = DataLoader(\n",
    "    OmniglotOriginDataset(\"../data/omniglot_mini/images_background/Japanese_(hiragana)/\", \n",
    "                    n_class=n_class,\n",
    "                    train=True,\n",
    "                    train_index=[0],\n",
    "                    transform=transforms.Compose([\n",
    "                        ToTensor()\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "local_task_test_data_loader = DataLoader(\n",
    "    OmniglotOriginDataset(\"../data/omniglot_mini/images_background/Japanese_(hiragana)/\", \n",
    "                    n_class=n_class,\n",
    "                    train=False,\n",
    "                    train_index=[0],\n",
    "                    transform=transforms.Compose([\n",
    "                        ToTensor()\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = OmniglotNet(n_class=10).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "with tqdm(range(10)) as _tqdm:\n",
    "    for epoch in _tqdm:\n",
    "        train_loss, train_acc = train(model, device, local_task_train_data_loader, optimizer, epoch)\n",
    "        test_loss, test_acc = test(model, device, local_task_test_data_loader)\n",
    "        _tqdm.set_postfix(OrderedDict(\n",
    "            epoch=epoch+1, \n",
    "            train_loss=train_loss, train_acc=train_acc, \n",
    "            test_loss=test_loss, test_acc=test_acc))\n",
    "\n",
    "data, target = local_task_train_data_loader.__iter__().next()\n",
    "\n",
    "images = np.array(data).reshape(10,28,28)\n",
    "plt.figure(figsize=(10,1))\n",
    "[[plt.subplot(1,10,i+1), plt.imshow(img)] for i, img in enumerate(images)]; plt.show()\n",
    "\n",
    "print(\"y_pred:\", torch.argmax(model(data.cuda()), 1).cpu())\n",
    "print(\"y_true:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taskset and TaskLoader\n",
    "- original classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taskset(object):\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TaskLoader(object):\n",
    "    def __init__(self, taskset, shuffle=True):\n",
    "        self.taskset = taskset\n",
    "        self.sample_iter = iter(np.random.permutation(np.arange(len(taskset))))\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        return self.taskset[next(self.sample_iter)]\n",
    "    def __len__(self):\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taskset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotTaskset(Taskset):\n",
    "    def __init__(self, path_to_omniglot, n_class, n_shot, meta_train):\n",
    "        \n",
    "        if meta_train:\n",
    "            path_to_lang = os.path.join(path_to_omniglot, \"images_background/\")\n",
    "        else:\n",
    "            path_to_lang = os.path.join(path_to_omniglot, \"images_evaluation/\")\n",
    "        \n",
    "        langs = sorted(os.listdir(path_to_lang))\n",
    "        tasks = [os.path.join(path_to_lang, lang) for lang in langs]\n",
    "        tasks = [task for task in tasks if len(os.listdir(task))>=n_class]\n",
    "        self.tasks = tasks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tasks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        train_index=[np.random.randint(20)] #TODO chanege indices\n",
    "        return {\"train\":\n",
    "                DataLoader(\n",
    "                    OmniglotOriginDataset(self.tasks[idx], \n",
    "                                    n_class=n_class,\n",
    "                                    train=True,\n",
    "                                    train_index=train_index,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        ToTensor()\n",
    "                                    ])),\n",
    "                    batch_size=batch_size, shuffle=True), \n",
    "                \"test\":\n",
    "                DataLoader(\n",
    "                    OmniglotOriginDataset(self.tasks[idx],\n",
    "                                    n_class=n_class,\n",
    "                                    train=False,\n",
    "                                    train_index=train_index,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        ToTensor()\n",
    "                                    ])),\n",
    "                    batch_size=batch_size, shuffle=True),\n",
    "                \"task\": self.tasks[idx] \n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TaskLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "../data/omniglot_mini/images_background/Burmese_(Myanmar)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 0, 9, 2, 3, 4, 7, 1, 6, 5])\n",
      "../data/omniglot_mini/images_background/Braille\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 7, 3, 5, 1, 2, 0, 4, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "meta_train_task_loader = TaskLoader(\n",
    "    OmniglotTaskset(\"../data/omniglot_mini/\", meta_train=True, n_class=n_class, n_shot=n_shot)\n",
    ")\n",
    "\n",
    "print(len(meta_train_task_loader.taskset))\n",
    "\n",
    "for i, meta_train_task in enumerate(meta_train_task_loader):\n",
    "    print(meta_train_task[\"task\"])\n",
    "    local_task_train_data_loader = meta_train_task[\"train\"]\n",
    "    for data, target in local_task_train_data_loader:\n",
    "        plt.figure(figsize=(10,1))\n",
    "        for j, x in enumerate(data):\n",
    "            plt.subplot(1, batch_size, j+1); plt.imshow(x[0])\n",
    "        plt.show()\n",
    "        print(target)\n",
    "    if i==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class MetaLearner(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 0.1\n",
    "        self.momentum = 0.5\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.master_net = OmniglotNet(n_class).to(self.device)\n",
    "        self.master_opt = optim.Adam(self.master_net.parameters(), lr=0.001)\n",
    "        self.keys = self.master_net.state_dict().keys()\n",
    "    \n",
    "    #TODO need return or not\n",
    "    def copy_params(self, from_net, to_net):\n",
    "        params = {k: v for k, v in from_net.state_dict().items() if k in self.keys}\n",
    "        to_net.load_state_dict(params, strict=False)\n",
    "    \n",
    "    def meta_test(self):\n",
    "        \n",
    "        meta_test_task_loader = TaskLoader(\n",
    "#             OmniglotTaskset(\"../data/omniglot_mini/\", meta_train=True, n_class=n_class, n_shot=n_shot))\n",
    "            OmniglotTaskset(\"../data/omniglot_mini/\", meta_train=False, n_class=n_class, n_shot=n_shot))\n",
    "\n",
    "        loss, acc = [], []\n",
    "        \n",
    "        for meta_test_task in meta_test_task_loader:\n",
    "            \n",
    "            # copy master model to new branch model\n",
    "            faster_net = OmniglotNet(n_class).to(self.device)\n",
    "            self.copy_params(self.master_net, faster_net)\n",
    "            faster_opt = optim.SGD(faster_net.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "            \n",
    "            # make local task data loader\n",
    "            local_task_train_data_loader = meta_test_task[\"train\"]\n",
    "            local_task_test_data_loader = meta_test_task[\"test\"]\n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # meta test task train\n",
    "            # ----------------------------------------------------------------\n",
    "            \n",
    "            for epoch in range(n_local_update):################################?????n_local_update+1000\n",
    "                train(faster_net, self.device, local_task_train_data_loader, faster_opt, epoch)\n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # meta test task test\n",
    "            # ----------------------------------------------------------------\n",
    "            \n",
    "            _loss, _acc = test(faster_net, self.device, local_task_test_data_loader)\n",
    "            loss.append(_loss)\n",
    "            acc.append(_acc)\n",
    "            \n",
    "        print(\"meta_test_loss: {:.6f}, meta_test_acc: {:.6f}\".format(\n",
    "            np.mean(loss), np.mean(acc)))\n",
    "        \n",
    "    \n",
    "    def meta_train(self):\n",
    "        \n",
    "        meta_train_task_loader = TaskLoader(\n",
    "            OmniglotTaskset(\"../data/omniglot_mini/\", meta_train=True, n_class=n_class, n_shot=n_shot))\n",
    "        \n",
    "        \n",
    "        for meta_train_task in meta_train_task_loader:\n",
    "            \n",
    "            meta_grads = []\n",
    "            \n",
    "            \n",
    "            # copy master model to new branch model\n",
    "            faster_net = OmniglotNet(n_class).to(self.device)\n",
    "            self.copy_params(self.master_net, faster_net)\n",
    "            \n",
    "            faster_params = OrderedDict((name, param) for (name, param) in faster_net.named_parameters())\n",
    "            master_params = OrderedDict((name, param) for (name, param) in self.master_net.named_parameters())\n",
    "        \n",
    "            # make local task data loader\n",
    "            local_task_train_data_loader = meta_train_task[\"train\"]\n",
    "            local_task_test_data_loader = meta_train_task[\"test\"]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             for name, param in self.master_net.named_parameters():\n",
    "#                 print(name, param[0].grad)\n",
    "#                 break\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "             \n",
    "            # ----------------------------------------------------------------\n",
    "            # meta train task test pre1\n",
    "            # ----------------------------------------------------------------\n",
    "            \n",
    "            pre_test_loss = []\n",
    "        \n",
    "            for data, target in local_task_test_data_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                output = faster_net.manual_forward(data, faster_params)\n",
    "                \n",
    "#                 print(output[0])\n",
    "                \n",
    "#                 output_ = self.master_net(data)\n",
    "                \n",
    "#                 print(output_[0])\n",
    "                \n",
    "                \n",
    "                loss = F.nll_loss(output, target)\n",
    "                pre_test_loss.append(loss.item())\n",
    "                \n",
    "            pre_test_loss = np.mean(pre_test_loss)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # meta train task train\n",
    "            # ----------------------------------------------------------------\n",
    "\n",
    "#             print(\"\\n#\",\"-\"*60)\n",
    "#             print(\"# meta train task train\")\n",
    "#             print(\"#\",\"-\"*60, \"\\n\")\n",
    "\n",
    "            # train n_local_update times / 1 task\n",
    "            first_train_for_this_task = True\n",
    "            \n",
    "            for epoch in range(n_local_update):\n",
    "                \n",
    "                for data, target in local_task_train_data_loader:\n",
    "                    \n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    \n",
    "                    if first_train_for_this_task:\n",
    "                        # manual predict\n",
    "                        output = self.master_net(data) # あとでmaster_netのパラメタで微分するので\n",
    "                        loss = F.nll_loss(output, target)\n",
    "\n",
    "                        # manual optimize\n",
    "#                         grads = torch.autograd.grad(loss, self.master_net.parameters())\n",
    "                        grads = torch.autograd.grad(loss, self.master_net.parameters(), create_graph=True)\n",
    "                        \n",
    "                        first_train_for_this_task = False\n",
    "                    \n",
    "                    else:\n",
    "                        # manual predict\n",
    "                        output = faster_net.manual_forward(data, faster_params)\n",
    "                        loss = F.nll_loss(output, target)\n",
    "                        \n",
    "#                         if epoch>0:\n",
    "#                             print(epoch+1, loss.item())\n",
    "                        # manual optimize\n",
    "#                         grads = torch.autograd.grad(loss, faster_params.values())\n",
    "                        grads = torch.autograd.grad(loss, faster_params.values(), create_graph=True)\n",
    "                    \n",
    "                    faster_params = OrderedDict(\n",
    "                        (name, param - self.lr*grad)\n",
    "                        for ((name, param), grad) in zip(faster_params.items(), grads)\n",
    "                    )\n",
    "            \n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # meta train task test\n",
    "            # ----------------------------------------------------------------\n",
    "            \n",
    "            \n",
    "#             print(\"\\n#\",\"-\"*60)\n",
    "#             print(\"# meta train task test\")\n",
    "#             print(\"#\",\"-\"*60, \"\\n\")\n",
    "            \n",
    "            post_test_loss = []\n",
    "            \n",
    "            for data, target in local_task_test_data_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                output = faster_net.manual_forward(data, faster_params)\n",
    "                loss = F.nll_loss(output, target) # test_loss計算するとこまではfaster_net\n",
    "                \n",
    "                post_test_loss.append(loss.item())\n",
    "                \n",
    "                # manual optimize!!!\n",
    "                \n",
    "                # test_lossの微分はmaster_net\n",
    "                grads = torch.autograd.grad(loss, self.master_net.parameters(), create_graph=True)\n",
    "                \n",
    "                grads = {name:g for ((name, _), g) in zip(faster_net.named_parameters(), grads)} # ??\n",
    "                \n",
    "                meta_grads.append(grads) # TODO normalize grads by local_task_test_data_size\n",
    "                                \n",
    "                pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            post_test_loss = np.mean(post_test_loss)\n",
    "            \n",
    "#             print(\"pre - post ==\", pre_test_loss-post_test_loss)\n",
    "        \n",
    "        \n",
    "            meta_grads = {k: sum(grads[k] for grads in meta_grads) for k in meta_grads[0].keys()}\n",
    "\n",
    "            dumy_data_loader = DataLoader(\n",
    "                OmniglotOriginDataset(\"../data/omniglot_mini/images_background/Japanese_(hiragana)/\", \n",
    "                                n_class=n_class,\n",
    "                                train=False,\n",
    "                                train_index=[0],\n",
    "                                transform=transforms.Compose([\n",
    "                                    ToTensor()\n",
    "                                ])),\n",
    "                batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            data, target = dumy_data_loader.__iter__().next()\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "    #         output = self.master_net.manual_forward(data, master_params) ##########not manual\n",
    "            output = self.master_net(data) ##########not manual\n",
    "            loss = F.nll_loss(output, target)\n",
    "\n",
    "            self.master_opt.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "    #         for name, param in self.master_net.named_parameters():\n",
    "    #             print(\"\\nbbb\", name, param.grad[0], \"\\n\")\n",
    "    #             print(param[0])\n",
    "    #             print()\n",
    "    #             break\n",
    "\n",
    "\n",
    "            hooks = []\n",
    "            for (k,v) in self.master_net.named_parameters():\n",
    "                def get_closure():\n",
    "                    key = k\n",
    "                    def replace_grad(grad):\n",
    "                        return meta_grads[key]\n",
    "                    return replace_grad\n",
    "                hooks.append(v.register_hook(get_closure()))\n",
    "    #         print(hooks)\n",
    "\n",
    "            # Compute grads for current step, replace with summed gradients as defined by hook\n",
    "            self.master_opt.zero_grad()\n",
    "\n",
    "    #         for name, param in self.master_net.named_parameters():\n",
    "    #             print(\"\\nccc\", name, param.grad[0], \"\\n\")\n",
    "    #             print(param[0])\n",
    "    #             print()\n",
    "    #             break\n",
    "\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "    #         for name, param in self.master_net.named_parameters():\n",
    "    #             print(\"\\nddd\", name, param.grad[0], \"\\n\") # ちゃんとgradが書き換わっている。\n",
    "    #             print(param[0]) #まだopt.stepしてないのでパラメータは変わっていない\n",
    "    #             pre_param = deepcopy(param)\n",
    "    #             print()\n",
    "    #             break\n",
    "\n",
    "\n",
    "\n",
    "            # Update the net parameters with the accumulated gradient according to optimizer\n",
    "            self.master_opt.step()\n",
    "\n",
    "    #         for name, param in self.master_net.named_parameters():\n",
    "    #             print(\"\\neee\", name, param.grad[0], \"\\n\") # ちゃんとgradが書き換わっている。\n",
    "    #             print(param[0]) # 0.001単位の更新しか行われていない\n",
    "    #             post_param = deepcopy(param)\n",
    "    #             print()\n",
    "    #             break\n",
    "\n",
    "    #         print(\"\\nfff\", pre_param-post_param, \"\\n\")\n",
    "\n",
    "            # Remove the hooks before next training phase\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "\n",
    "        # ----------------------------------------------------------------\n",
    "        # end all tasks\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "            \n",
    "            \n",
    "#         for name, param in self.master_net.named_parameters():\n",
    "#             print(\"\\naaa\", name, param.grad, \"\\n\")\n",
    "#             print(param[0])\n",
    "#             print()\n",
    "#             break\n",
    "        \n",
    "        # summing up grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre - post == 0.48579924984982137\n",
      "pre - post == 0.22278807037755044\n",
      "pre - post == 0.4426850143231842\n",
      "pre - post == 0.2673594763404443\n",
      "pre - post == 0.15015066297430746\n",
      "pre - post == 0.4919279625541284\n",
      "pre - post == 0.30369786839736124\n",
      "pre - post == 0.4227453972163955\n",
      "pre - post == 0.5214306806263169\n",
      "pre - post == 0.5173174268320986\n",
      "pre - post == 0.4782469335355257\n",
      "pre - post == 0.28437701651924563\n",
      "pre - post == 0.34244680404663086\n",
      "pre - post == 0.19338950985356362\n",
      "pre - post == 0.26085881183021975\n",
      "pre - post == 0.5862238783585401\n",
      "pre - post == 0.4023972749710085\n",
      "pre - post == 0.31705419013374714\n",
      "pre - post == 0.6036753905446905\n",
      "pre - post == 0.41456915830311036\n",
      "pre - post == 0.717934244557431\n",
      "pre - post == 0.3654957884236385\n",
      "pre - post == 0.3366384757192511\n",
      "pre - post == 0.5005089044570921\n",
      "pre - post == 0.3097909751691317\n",
      "pre - post == 0.2314568130593555\n",
      "pre - post == 0.48566805688958414\n",
      "pre - post == 0.3575143876828646\n",
      "pre - post == 0.325068994572288\n",
      "pre - post == 0.2887323341871566\n",
      "meta_test_loss: 1.916351, meta_test_acc: 0.414474\n",
      "pre - post == 0.353281993615\n",
      "pre - post == 0.21000710286592206\n",
      "pre - post == 0.23526640314804848\n",
      "pre - post == 0.6670952094228644\n",
      "pre - post == 0.6193797839315314\n",
      "pre - post == 0.45771243697718567\n",
      "pre - post == 0.525663965626767\n",
      "pre - post == 0.2993716064252352\n",
      "pre - post == 0.34980210505033793\n",
      "pre - post == 0.33107353511609516\n",
      "pre - post == 0.5933160405409965\n",
      "pre - post == 0.31140818093952394\n",
      "pre - post == 0.3347942013489571\n",
      "pre - post == 0.2698926611950525\n",
      "pre - post == 0.4873224810550085\n",
      "pre - post == 0.5585263779288845\n",
      "pre - post == 0.34400047753986573\n",
      "pre - post == 0.5027902126312256\n",
      "pre - post == 0.561833281266062\n",
      "pre - post == 0.41118531478078735\n",
      "pre - post == 0.5238495876914575\n",
      "pre - post == 0.5543826128307143\n",
      "pre - post == 0.13830264618522214\n",
      "pre - post == 0.39967920905665366\n",
      "pre - post == 0.24726965552882163\n",
      "pre - post == 0.2125506150095089\n",
      "pre - post == 0.5623084557683844\n",
      "pre - post == 0.5254804523367631\n",
      "pre - post == 0.3402464640767948\n",
      "pre - post == 0.43625465819710185\n",
      "meta_test_loss: 1.903054, meta_test_acc: 0.421053\n",
      "pre - post == 0.4829586556083276\n",
      "pre - post == 0.43084152748710247\n",
      "pre - post == 0.35151020476692607\n",
      "pre - post == 0.23888879700710897\n",
      "pre - post == 0.687428505797135\n",
      "pre - post == 0.5178190532483553\n",
      "pre - post == 0.48257241751018354\n",
      "pre - post == 0.5301012741891962\n",
      "pre - post == 0.25948085910395546\n",
      "pre - post == 0.5368381613179258\n",
      "pre - post == 0.33094106849871174\n",
      "pre - post == 0.4293554456610429\n",
      "pre - post == 0.49915964352457154\n",
      "pre - post == 0.3255764434212134\n",
      "pre - post == 0.43667414941285787\n",
      "pre - post == 0.5696318086824921\n",
      "pre - post == 0.21205761558131186\n",
      "pre - post == 0.4330280580018697\n",
      "pre - post == 0.3201116511696265\n",
      "pre - post == 0.5777121531335929\n",
      "pre - post == 0.3288859191693758\n",
      "pre - post == 0.42736685276031494\n",
      "pre - post == 0.5389144608848973\n",
      "pre - post == 0.3445460294422351\n",
      "pre - post == 0.3229328205710966\n",
      "pre - post == 0.6312518684487594\n",
      "pre - post == 0.34027031220887816\n",
      "pre - post == 0.5380320360786037\n",
      "pre - post == 0.4184377444417855\n",
      "pre - post == 0.5181364385705245\n",
      "meta_test_loss: 1.882162, meta_test_acc: 0.424737\n",
      "pre - post == 0.4719356361188387\n",
      "pre - post == 0.294500419968053\n",
      "pre - post == 0.5267873563264545\n",
      "pre - post == 0.40375577148638264\n",
      "pre - post == 0.6834546202107479\n",
      "pre - post == 0.22839855520348795\n",
      "pre - post == 0.4756139391346983\n",
      "pre - post == 0.7516471523987618\n",
      "pre - post == 0.552744557982997\n",
      "pre - post == 0.28959481339705606\n",
      "pre - post == 0.5201474428176878\n",
      "pre - post == 0.536710826974166\n",
      "pre - post == 0.399941136962489\n",
      "pre - post == 0.3150893638008516\n",
      "pre - post == 0.588678341162832\n",
      "pre - post == 0.41556951874180825\n",
      "pre - post == 0.5830801223453721\n",
      "pre - post == 0.5376316685425606\n",
      "pre - post == 0.5593894782819246\n",
      "pre - post == 0.46526957813062153\n",
      "pre - post == 0.3742328443025287\n",
      "pre - post == 0.40041411550421446\n",
      "pre - post == 0.2847791596462854\n",
      "pre - post == 0.4528932194960744\n",
      "pre - post == 0.3179104516380711\n",
      "pre - post == 0.49381275553452353\n",
      "pre - post == 0.5597171030546491\n",
      "pre - post == 0.5314546196084275\n",
      "pre - post == 0.4758378267288208\n",
      "pre - post == 0.3755317110764351\n",
      "meta_test_loss: 1.898164, meta_test_acc: 0.400789\n",
      "pre - post == 0.6245893239974976\n",
      "pre - post == 0.5934312155372219\n",
      "pre - post == 0.3632538067667106\n",
      "pre - post == 0.5547747298290855\n",
      "pre - post == 0.5418896926076788\n",
      "pre - post == 0.5333968714663855\n",
      "pre - post == 0.6137311772296303\n",
      "pre - post == 0.43294809366527387\n",
      "pre - post == 0.4703941847148696\n",
      "pre - post == 0.363451681639019\n",
      "pre - post == 0.5723666831066734\n",
      "pre - post == 0.7141091509869224\n",
      "pre - post == 0.36411836900209105\n",
      "pre - post == 0.533910218038057\n",
      "pre - post == 0.4966912834267865\n",
      "pre - post == 0.8509951955393742\n",
      "pre - post == 0.34729612501044027\n",
      "pre - post == 0.34133525898582073\n",
      "pre - post == 0.8526693708018254\n",
      "pre - post == 0.3531377064554315\n",
      "pre - post == 0.32066118089776285\n",
      "pre - post == 0.25582080138357055\n",
      "pre - post == 0.5193448066711428\n",
      "pre - post == 0.664120454537241\n",
      "pre - post == 0.43901134792127094\n",
      "pre - post == 0.3231576430170162\n",
      "pre - post == 0.47805021311107443\n",
      "pre - post == 0.4523947866339433\n",
      "pre - post == 0.5250029626645543\n",
      "pre - post == 0.42988378750650513\n",
      "meta_test_loss: 1.832402, meta_test_acc: 0.444737\n",
      "pre - post == 0.5208353996276855\n",
      "pre - post == 0.4252862553847463\n",
      "pre - post == 0.27876724067487224\n",
      "pre - post == 0.5677742016942879\n",
      "pre - post == 0.4177080518320986\n",
      "pre - post == 0.34290151847036254\n",
      "pre - post == 0.4964017554333333\n",
      "pre - post == 0.3976810919611078\n",
      "pre - post == 0.30294921523646323\n",
      "pre - post == 0.5807132281755147\n",
      "pre - post == 0.40372933212079487\n",
      "pre - post == 0.35228211001345944\n",
      "pre - post == 0.3806351297780086\n",
      "pre - post == 0.5645731687545774\n",
      "pre - post == 0.802404052332828\n",
      "pre - post == 0.7424713059475547\n",
      "pre - post == 0.6653204968101099\n",
      "pre - post == 0.33740021680530763\n",
      "pre - post == 0.5394537762591711\n",
      "pre - post == 0.6398734795419794\n",
      "pre - post == 0.49249633989836017\n",
      "pre - post == 0.4552104347630552\n",
      "pre - post == 0.644235240785699\n",
      "pre - post == 0.7536403693650897\n",
      "pre - post == 0.28881261223240884\n",
      "pre - post == 0.5579115403325934\n",
      "pre - post == 0.4359763296026933\n",
      "pre - post == 0.39786102269825174\n",
      "pre - post == 0.29365880865799765\n",
      "pre - post == 0.46809637546539307\n",
      "meta_test_loss: 1.763367, meta_test_acc: 0.494211\n",
      "pre - post == 0.36388585441990906\n",
      "pre - post == 0.3911029539610211\n",
      "pre - post == 0.6600579525295054\n",
      "pre - post == 0.6799531924097162\n",
      "pre - post == 0.48074455637680846\n",
      "pre - post == 0.8428600524601186\n",
      "pre - post == 0.3484692134355243\n",
      "pre - post == 0.3835942368758354\n",
      "pre - post == 0.5021472039975621\n",
      "pre - post == 0.3576213246897648\n",
      "pre - post == 0.556568183396992\n",
      "pre - post == 0.33429573084178754\n",
      "pre - post == 0.5375823472675525\n",
      "pre - post == 0.44080519676208496\n",
      "pre - post == 0.4531903957065784\n",
      "pre - post == 0.6194770022442466\n",
      "pre - post == 0.846651127463893\n",
      "pre - post == 0.6323265966616178\n",
      "pre - post == 0.7001327276229856\n",
      "pre - post == 0.6896421595623621\n",
      "pre - post == 0.6160566681309749\n",
      "pre - post == 0.487676369516473\n",
      "pre - post == 0.5915239798395255\n",
      "pre - post == 0.4197626804050647\n",
      "pre - post == 0.28939504999863486\n",
      "pre - post == 0.12428380313672527\n",
      "pre - post == 0.5902410243686878\n",
      "pre - post == 0.4823475636933978\n",
      "pre - post == 0.4733179807662966\n",
      "pre - post == 0.656880159127085\n",
      "meta_test_loss: 1.774079, meta_test_acc: 0.464211\n",
      "pre - post == 0.7394135249288458\n",
      "pre - post == 0.7613260118584886\n",
      "pre - post == 0.7605565786361694\n",
      "pre - post == 0.47848262284931375\n",
      "pre - post == 0.6899513947336298\n",
      "pre - post == 0.3371439733003314\n",
      "pre - post == 0.51434904650638\n",
      "pre - post == 0.5056102087623193\n",
      "pre - post == 0.5594846135691594\n",
      "pre - post == 0.4650738239288328\n",
      "pre - post == 0.6082963316064132\n",
      "pre - post == 0.6137846080880418\n",
      "pre - post == 0.4296119903263296\n",
      "pre - post == 0.5413540538988615\n",
      "pre - post == 0.6983130291888588\n",
      "pre - post == 0.6396134589847766\n",
      "pre - post == 0.33292641137775614\n",
      "pre - post == 0.6034759471290989\n",
      "pre - post == 0.5831724718997353\n",
      "pre - post == 0.5289539224223085\n",
      "pre - post == 0.2539382294604655\n",
      "pre - post == 0.8656003475189211\n",
      "pre - post == 0.28362409692061563\n",
      "pre - post == 0.5872225447704917\n",
      "pre - post == 0.5224443423120599\n",
      "pre - post == 0.31513242972524536\n",
      "pre - post == 0.44999480874914877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre - post == 0.7650131300876015\n",
      "pre - post == 0.7662200614025718\n",
      "pre - post == 0.4940269746278463\n",
      "meta_test_loss: 1.711536, meta_test_acc: 0.518421\n",
      "pre - post == 0.6538712664654378\n",
      "pre - post == 0.48690298356507955\n",
      "pre - post == 0.4594503954837197\n",
      "pre - post == 0.608221123093053\n",
      "pre - post == 0.7587427340055766\n",
      "pre - post == 0.7961129075602482\n",
      "pre - post == 0.6888739435296309\n",
      "pre - post == 0.5068722524141009\n",
      "pre - post == 0.5520982114892257\n",
      "pre - post == 0.6061445976558484\n",
      "pre - post == 0.46301186084747314\n",
      "pre - post == 0.4229856478540521\n",
      "pre - post == 0.5222757113607306\n",
      "pre - post == 0.38758803668775066\n",
      "pre - post == 0.48662855750636047\n",
      "pre - post == 0.42197892540379556\n",
      "pre - post == 0.6848434272565342\n",
      "pre - post == 0.6761357847012972\n",
      "pre - post == 0.5175593965931944\n",
      "pre - post == 0.8819524865401418\n",
      "pre - post == 0.8996208780690245\n",
      "pre - post == 0.35937670030091917\n",
      "pre - post == 0.5350132427717511\n",
      "pre - post == 0.2695219328528955\n",
      "pre - post == 0.7572795529114571\n",
      "pre - post == 0.6896922337381464\n",
      "pre - post == 0.42755890519995443\n",
      "pre - post == 0.44527249587209616\n",
      "pre - post == 0.7254556731173865\n",
      "pre - post == 0.13993140270835447\n",
      "meta_test_loss: 1.702079, meta_test_acc: 0.500000\n",
      "pre - post == 0.34029077856164247\n",
      "pre - post == 0.5618471283661692\n",
      "pre - post == 0.7254052915071187\n",
      "pre - post == 0.34725826037557517\n",
      "pre - post == 0.646183220963729\n",
      "pre - post == 0.4171264046116878\n",
      "pre - post == 0.3641539937571474\n",
      "pre - post == 0.5641373144952875\n",
      "pre - post == 0.4571358090952824\n",
      "pre - post == 0.5241634092832868\n",
      "pre - post == 0.8278324353067499\n",
      "pre - post == 0.6462939663937217\n",
      "pre - post == 0.41879337084920776\n",
      "pre - post == 0.8313212771164744\n",
      "pre - post == 0.4678280729996529\n",
      "pre - post == 0.46314048767089844\n",
      "pre - post == 0.6887347572728209\n",
      "pre - post == 0.7637675686886436\n",
      "pre - post == 0.9246805592587122\n",
      "pre - post == 0.5776735857913371\n",
      "pre - post == 0.684638180230793\n",
      "pre - post == 0.5829277603249798\n",
      "pre - post == 0.5872282417196977\n",
      "pre - post == 0.51459697045778\n",
      "pre - post == 0.4583308822230292\n",
      "pre - post == 0.576976594172026\n",
      "pre - post == 0.877987918100859\n",
      "pre - post == 0.7972882797843532\n",
      "pre - post == 0.5072521598715534\n",
      "pre - post == 0.5476106342516447\n",
      "meta_test_loss: 1.674729, meta_test_acc: 0.525263\n",
      "pre - post == 0.9029329826957302\n",
      "pre - post == 0.5770354710127179\n",
      "pre - post == 0.5479870971880461\n",
      "pre - post == 0.4479291878248517\n",
      "pre - post == 0.6156163592087596\n",
      "pre - post == 0.5022730576364618\n",
      "pre - post == 0.47642177657077167\n",
      "pre - post == 0.8335252310100354\n",
      "pre - post == 0.5349757294905813\n",
      "pre - post == 0.8159387425372475\n",
      "pre - post == 0.7738951695592782\n",
      "pre - post == 0.5823828358399241\n",
      "pre - post == 0.8433936583368402\n",
      "pre - post == 0.5561478639903823\n",
      "pre - post == 0.4472380562832483\n",
      "pre - post == 0.4628652773405375\n",
      "pre - post == 0.5672595438204315\n",
      "pre - post == 0.5317536529741789\n",
      "pre - post == 0.7433627843856814\n",
      "pre - post == 0.4363363479313098\n",
      "pre - post == 0.5500083157890723\n",
      "pre - post == 0.7490211035075942\n",
      "pre - post == 0.9010401148545117\n",
      "pre - post == 0.6304114115865609\n",
      "pre - post == 1.0010231482355219\n",
      "pre - post == 0.4772095805720278\n",
      "pre - post == 0.8055156281119897\n",
      "pre - post == 0.41299872021926065\n",
      "pre - post == 0.8013738895717422\n",
      "pre - post == 0.3283939738022654\n",
      "meta_test_loss: 1.662232, meta_test_acc: 0.530526\n",
      "pre - post == 0.4452357543142218\n",
      "pre - post == 0.7138958228261845\n",
      "pre - post == 0.43561932915135415\n",
      "pre - post == 0.5907582797502218\n",
      "pre - post == 0.47421582121598105\n",
      "pre - post == 0.5894387583983574\n",
      "pre - post == 0.8102204172234788\n",
      "pre - post == 0.6132506822284902\n",
      "pre - post == 0.5878084458802875\n",
      "pre - post == 0.646370856385482\n",
      "pre - post == 0.7915977427833958\n",
      "pre - post == 0.9364101322073688\n",
      "pre - post == 0.7440764904022217\n",
      "pre - post == 0.4030925286443612\n",
      "pre - post == 0.6517136159696075\n",
      "pre - post == 0.7613832511399923\n",
      "pre - post == 0.989347225741336\n",
      "pre - post == 0.9731945552323995\n",
      "pre - post == 0.6179232660092808\n",
      "pre - post == 0.5899793976231626\n",
      "pre - post == 0.9561047240307456\n",
      "pre - post == 0.7458850208081698\n",
      "pre - post == 0.6223265371824567\n",
      "pre - post == 0.38878036800183735\n",
      "pre - post == 0.6010245084762573\n",
      "pre - post == 0.4469301261399923\n",
      "pre - post == 0.7695038444117497\n",
      "pre - post == 0.5526082013782703\n",
      "pre - post == 0.8193654951296354\n",
      "pre - post == 0.3649610406474062\n",
      "meta_test_loss: 1.617792, meta_test_acc: 0.542632\n",
      "pre - post == 0.5913643774233368\n",
      "pre - post == 0.6565638529626947\n",
      "pre - post == 0.6923081247430098\n",
      "pre - post == 0.5598016349892867\n",
      "pre - post == 0.6400148115660014\n",
      "pre - post == 0.4605057615982857\n",
      "pre - post == 1.0115267979471305\n",
      "pre - post == 0.8025643825531006\n",
      "pre - post == 0.8259569720218056\n",
      "pre - post == 1.0282956048061975\n",
      "pre - post == 0.7787329523186934\n",
      "pre - post == 0.7870107073532908\n",
      "pre - post == 0.9180105548155935\n",
      "pre - post == 0.6701958242215609\n",
      "pre - post == 0.48813591505351805\n",
      "pre - post == 0.9513109357733476\n",
      "pre - post == 0.5771890627710443\n",
      "pre - post == 0.6503609230643823\n",
      "pre - post == 0.8510682269146568\n",
      "pre - post == 0.8839537720931203\n",
      "pre - post == 0.6719697211918079\n",
      "pre - post == 0.8253676828585172\n",
      "pre - post == 1.016770513434159\n",
      "pre - post == 0.6645589627717672\n",
      "pre - post == 0.4123897677973698\n",
      "pre - post == 0.6552940544329189\n",
      "pre - post == 0.4254664684596814\n",
      "pre - post == 0.8987949898368435\n",
      "pre - post == 0.6963890602714136\n",
      "pre - post == 0.5994229755903546\n",
      "meta_test_loss: 1.657746, meta_test_acc: 0.507368\n",
      "pre - post == 0.8093167668894716\n",
      "pre - post == 0.5920044497439736\n",
      "pre - post == 0.3950525145781665\n",
      "pre - post == 0.7957503105464736\n",
      "pre - post == 0.6792305707931519\n",
      "pre - post == 1.1576726562098454\n",
      "pre - post == 0.519999077445582\n",
      "pre - post == 0.7578493042996055\n",
      "pre - post == 0.8552290640379252\n",
      "pre - post == 0.7013310131273773\n",
      "pre - post == 0.8955065262945072\n",
      "pre - post == 0.818057235918547\n",
      "pre - post == 0.6802374814686023\n",
      "pre - post == 0.43465059681942586\n",
      "pre - post == 0.5853763630515652\n",
      "pre - post == 0.5894554163280288\n",
      "pre - post == 0.6113443562858984\n",
      "pre - post == 0.6414436980297693\n",
      "pre - post == 1.0184392929077148\n",
      "pre - post == 1.0483486903341195\n",
      "pre - post == 0.807021994339792\n",
      "pre - post == 0.6600640572999652\n",
      "pre - post == 0.6108027006450452\n",
      "pre - post == 0.745013337386282\n",
      "pre - post == 1.1241133683606195\n",
      "pre - post == 0.9762459114978186\n",
      "pre - post == 0.5703213089390806\n",
      "pre - post == 1.025412691266913\n",
      "pre - post == 0.908827267195049\n",
      "pre - post == 0.809501742061816\n",
      "meta_test_loss: 1.649694, meta_test_acc: 0.500000\n",
      "pre - post == 0.8655857914372493\n",
      "pre - post == 0.7520099188152112\n",
      "pre - post == 1.1783308324060942\n",
      "pre - post == 0.611391299649289\n",
      "pre - post == 0.6970671666295907\n",
      "pre - post == 0.6060766922800165\n",
      "pre - post == 0.5663912359036896\n",
      "pre - post == 0.6430289117913495\n",
      "pre - post == 0.7355103680962012\n",
      "pre - post == 0.672404408454895\n",
      "pre - post == 0.852809943650898\n",
      "pre - post == 0.7639321778949939\n",
      "pre - post == 0.7268397431624563\n",
      "pre - post == 1.033320847310518\n",
      "pre - post == 0.8567029112263731\n",
      "pre - post == 0.6337946465140893\n",
      "pre - post == 1.0964245984428804\n",
      "pre - post == 0.8209760063572935\n",
      "pre - post == 0.7217952891399988\n",
      "pre - post == 0.7609008487902191\n",
      "pre - post == 0.6854781477074874\n",
      "pre - post == 1.0628491765574406\n",
      "pre - post == 0.8512369017851982\n",
      "pre - post == 0.8990173339843752\n",
      "pre - post == 0.683036484216389\n",
      "pre - post == 0.6740076165450248\n",
      "pre - post == 1.2286235752858612\n",
      "pre - post == 0.9340101229517082\n",
      "pre - post == 0.7611445627714457\n",
      "pre - post == 0.7742478157344617\n",
      "meta_test_loss: 1.544989, meta_test_acc: 0.559737\n",
      "pre - post == 0.8798434106927167\n",
      "pre - post == 0.9429436859331635\n",
      "pre - post == 0.542317917472438\n",
      "pre - post == 0.7556789046839665\n",
      "pre - post == 0.5723357451589484\n",
      "pre - post == 1.0711467736645748\n",
      "pre - post == 1.1942748270536725\n",
      "pre - post == 0.6571756475850155\n",
      "pre - post == 0.6529625528737117\n",
      "pre - post == 0.7144353703448647\n",
      "pre - post == 0.6146353608683535\n",
      "pre - post == 0.9661904134248431\n",
      "pre - post == 0.8078065420451916\n",
      "pre - post == 0.7215417623519897\n",
      "pre - post == 0.7070519171263041\n",
      "pre - post == 0.6933480501174929\n",
      "pre - post == 0.9245080759650783\n",
      "pre - post == 0.7253044467223317\n",
      "pre - post == 0.7256505112898979\n",
      "pre - post == 1.2089133482230336\n",
      "pre - post == 0.6154212198759379\n",
      "pre - post == 0.6758249245191876\n",
      "pre - post == 0.9196672502316925\n",
      "pre - post == 0.9048112128910264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre - post == 0.9798755896718876\n",
      "pre - post == 0.9484117031097412\n",
      "pre - post == 0.9971099276291695\n",
      "pre - post == 0.4758831262588501\n",
      "pre - post == 0.8426216589777096\n",
      "pre - post == 0.9681074995743602\n",
      "meta_test_loss: 1.508639, meta_test_acc: 0.571842\n",
      "pre - post == 0.712544786302667\n",
      "pre - post == 0.5031465793910781\n",
      "pre - post == 0.968815809802005\n",
      "pre - post == 0.688407496402138\n",
      "pre - post == 0.677694364597923\n",
      "pre - post == 0.7081192920082493\n",
      "pre - post == 0.9669889399879856\n",
      "pre - post == 0.7226541920712122\n",
      "pre - post == 0.8277156603963751\n",
      "pre - post == 1.1422229691555625\n",
      "pre - post == 1.0657596964585154\n",
      "pre - post == 0.5536123263208488\n",
      "pre - post == 0.7109262064883581\n",
      "pre - post == 1.1895621262098615\n",
      "pre - post == 0.6928771483270744\n",
      "pre - post == 0.7543022507115413\n",
      "pre - post == 1.1802472880012111\n",
      "pre - post == 0.5981704310366982\n",
      "pre - post == 0.5641220996254368\n",
      "pre - post == 0.7578039733987108\n",
      "pre - post == 1.2695112981294332\n",
      "pre - post == 1.011322341467205\n",
      "pre - post == 0.8271909324746385\n",
      "pre - post == 0.7805213300805345\n",
      "pre - post == 0.9408657111619649\n",
      "pre - post == 0.8610980322486477\n",
      "pre - post == 0.896237197675203\n",
      "pre - post == 1.112485226831938\n",
      "pre - post == 1.2259014280218827\n",
      "pre - post == 0.9055033106552928\n",
      "meta_test_loss: 1.550622, meta_test_acc: 0.547632\n",
      "pre - post == 0.8725499793102867\n",
      "pre - post == 0.7646161255083586\n",
      "pre - post == 0.6430984108071578\n",
      "pre - post == 1.0281721667239538\n",
      "pre - post == 0.9017691235793266\n",
      "pre - post == 0.5814497659080908\n",
      "pre - post == 0.8099167158729152\n",
      "pre - post == 1.1315105180991323\n",
      "pre - post == 1.174295939897236\n",
      "pre - post == 0.8696037656382509\n",
      "pre - post == 0.9100900323767414\n",
      "pre - post == 0.5929184712861713\n",
      "pre - post == 0.6134209256423149\n",
      "pre - post == 1.1363772153854372\n",
      "pre - post == 0.5747315005252236\n",
      "pre - post == 0.7891467997902317\n",
      "pre - post == 0.652995925200613\n",
      "pre - post == 1.22005012474562\n",
      "pre - post == 1.2340027438966852\n",
      "pre - post == 1.2818728936345956\n",
      "pre - post == 0.774040617440876\n",
      "pre - post == 0.9705807347046704\n",
      "pre - post == 1.2393073659194143\n",
      "pre - post == 0.9342975553713349\n",
      "pre - post == 1.0334671045604507\n",
      "pre - post == 0.907324100795545\n",
      "pre - post == 0.744207971974423\n",
      "pre - post == 1.0006264950099746\n",
      "pre - post == 1.1925086598647268\n",
      "pre - post == 0.9364398027721204\n",
      "meta_test_loss: 1.554768, meta_test_acc: 0.550789\n",
      "pre - post == 0.903044016737687\n",
      "pre - post == 0.9305908617220429\n",
      "pre - post == 1.170851293363069\n",
      "pre - post == 1.2023851808748747\n",
      "pre - post == 0.9559746290508069\n",
      "pre - post == 1.222582591207404\n",
      "pre - post == 1.0442492836400081\n",
      "pre - post == 0.878421193674991\n",
      "pre - post == 1.2171544275785746\n",
      "pre - post == 1.210695454948827\n",
      "pre - post == 1.0921511461860254\n",
      "pre - post == 1.0010001220201192\n",
      "pre - post == 1.4495494428433873\n",
      "pre - post == 1.5301016192687187\n",
      "pre - post == 1.531406104564667\n",
      "pre - post == 1.0588126308039614\n",
      "pre - post == 1.1096125652915552\n",
      "pre - post == 1.3535178021380778\n",
      "pre - post == 1.1373095512390137\n",
      "pre - post == 1.615067538462187\n",
      "pre - post == 1.3149105184956598\n",
      "pre - post == 1.3877624963459214\n",
      "pre - post == 1.4562784119656214\n",
      "pre - post == 1.4008484012202211\n",
      "pre - post == 1.6001432255694743\n",
      "pre - post == 1.088276825453106\n",
      "pre - post == 1.294631330590499\n",
      "pre - post == 1.3777812719345093\n",
      "pre - post == 1.9186970246465584\n",
      "pre - post == 1.3136240620362134\n",
      "meta_test_loss: 1.567092, meta_test_acc: 0.541316\n",
      "pre - post == 0.9993829099755538\n",
      "pre - post == 1.3531402223988582\n",
      "pre - post == 1.5794846509632314\n",
      "pre - post == 1.2744599267056111\n",
      "pre - post == 1.336731935802259\n",
      "pre - post == 1.242121658827129\n",
      "pre - post == 1.2653479011435256\n",
      "pre - post == 1.3005056067516927\n",
      "pre - post == 1.818734601924294\n",
      "pre - post == 1.365914087546499\n",
      "pre - post == 1.5116761985578036\n",
      "pre - post == 1.2112341429057873\n",
      "pre - post == 1.2325355190979808\n",
      "pre - post == 1.2805100804881044\n",
      "pre - post == 1.3908826112747192\n",
      "pre - post == 1.5061070793553404\n",
      "pre - post == 1.7347532448015714\n",
      "pre - post == 1.6466175882439862\n",
      "pre - post == 1.867903094542654\n",
      "pre - post == 2.0307525772797437\n",
      "pre - post == 1.4390567290155512\n",
      "pre - post == 1.0601996371620581\n",
      "pre - post == 1.5849366313532778\n",
      "pre - post == 1.4353771586167183\n",
      "pre - post == 1.6589334701236924\n",
      "pre - post == 1.697305666772943\n",
      "pre - post == 1.2473537357229936\n",
      "pre - post == 1.6035682904092887\n",
      "pre - post == 1.3518595820979067\n",
      "pre - post == 1.8067720469675566\n",
      "meta_test_loss: 1.529877, meta_test_acc: 0.550789\n",
      "pre - post == 1.752237978734468\n",
      "pre - post == 1.464394538026107\n",
      "pre - post == 1.2957575007488853\n",
      "pre - post == 1.8785184433585718\n",
      "pre - post == 1.6835992399014925\n",
      "pre - post == 1.3713316039035195\n",
      "pre - post == 1.7234813351380196\n",
      "pre - post == 1.632512086316159\n",
      "pre - post == 1.3810091897060994\n",
      "pre - post == 1.7745482984342071\n",
      "pre - post == 1.7812620777832835\n",
      "pre - post == 1.7397045712721977\n",
      "pre - post == 1.660193117041337\n",
      "pre - post == 1.5207207202911375\n",
      "pre - post == 1.83631978536907\n",
      "pre - post == 1.888813185064416\n",
      "pre - post == 1.6875442768398086\n",
      "pre - post == 1.3499564685319598\n",
      "pre - post == 1.2783326161535162\n",
      "pre - post == 2.0283313079884175\n",
      "pre - post == 1.390890410071925\n",
      "pre - post == 1.7374804208153172\n",
      "pre - post == 1.5485312436756336\n",
      "pre - post == 1.9312251900371753\n",
      "pre - post == 2.120161969410746\n",
      "pre - post == 1.6066602092040214\n",
      "pre - post == 1.4173870337636845\n",
      "pre - post == 1.4132195711135864\n",
      "pre - post == 1.396164335702595\n",
      "pre - post == 1.4893109798431396\n",
      "meta_test_loss: 1.522791, meta_test_acc: 0.543421\n",
      "pre - post == 1.3919432539688914\n",
      "pre - post == 1.6779968173880326\n",
      "pre - post == 1.4101349178113436\n",
      "pre - post == 1.3523758210633932\n",
      "pre - post == 1.561804909455149\n",
      "pre - post == 1.4833804431714508\n",
      "pre - post == 1.8347854739741274\n",
      "pre - post == 1.5966209612394633\n",
      "pre - post == 1.246431457368951\n",
      "pre - post == 1.560440703442222\n",
      "pre - post == 1.4422327342786287\n",
      "pre - post == 1.7739941320921244\n",
      "pre - post == 1.3182606006923472\n",
      "pre - post == 1.8774981686943455\n",
      "pre - post == 1.5332381223377431\n",
      "pre - post == 1.9100939098157381\n",
      "pre - post == 1.611459826168261\n",
      "pre - post == 1.4021292422947131\n",
      "pre - post == 1.528547575599269\n",
      "pre - post == 1.5551953064767936\n",
      "pre - post == 1.5252516457909033\n",
      "pre - post == 2.080421108948557\n",
      "pre - post == 1.2062564021662665\n",
      "pre - post == 1.6169158596741526\n",
      "pre - post == 1.8631865005744133\n",
      "pre - post == 1.7828257083892824\n",
      "pre - post == 1.6619168582715484\n",
      "pre - post == 1.8403978159553127\n",
      "pre - post == 1.5309554338455202\n",
      "pre - post == 1.8773083247636495\n",
      "meta_test_loss: 1.524610, meta_test_acc: 0.535263\n",
      "pre - post == 1.749041381635164\n",
      "pre - post == 2.0281971504813745\n",
      "pre - post == 1.3156623714848568\n",
      "pre - post == 1.6289897027768587\n",
      "pre - post == 2.10525591749894\n",
      "pre - post == 1.4239173060969303\n",
      "pre - post == 1.4381529531980817\n",
      "pre - post == 1.5715054399088808\n",
      "pre - post == 1.6330338591023494\n",
      "pre - post == 1.3101750047583332\n",
      "pre - post == 1.276530799112822\n",
      "pre - post == 1.5467379783329211\n",
      "pre - post == 1.6952032917424253\n",
      "pre - post == 1.6987085781599345\n",
      "pre - post == 1.656579463105453\n",
      "pre - post == 1.5115175309934117\n",
      "pre - post == 1.5090300534900867\n",
      "pre - post == 1.3261638628809076\n",
      "pre - post == 1.3164341386995817\n",
      "pre - post == 1.7597875626463642\n",
      "pre - post == 1.4569393898311411\n",
      "pre - post == 1.2588859043623273\n",
      "pre - post == 1.5887949905897443\n",
      "pre - post == 1.6335543205863552\n",
      "pre - post == 1.9655037741912038\n",
      "pre - post == 1.4448399418278746\n",
      "pre - post == 1.425999917482075\n",
      "pre - post == 1.7375725758703133\n",
      "pre - post == 1.8544559290534572\n",
      "pre - post == 1.615657599348771\n",
      "meta_test_loss: 1.509457, meta_test_acc: 0.547895\n",
      "pre - post == 1.6657448379616988\n",
      "pre - post == 1.6199648192054346\n",
      "pre - post == 1.3567982661096671\n",
      "pre - post == 1.485425867532429\n",
      "pre - post == 1.5495384994306063\n",
      "pre - post == 1.6141432084535297\n",
      "pre - post == 1.4976218060443274\n",
      "pre - post == 1.8214474665491207\n",
      "pre - post == 1.3954077582610283\n",
      "pre - post == 1.6478524082585384\n",
      "pre - post == 1.6477658936851902\n",
      "pre - post == 1.5994306928233097\n",
      "pre - post == 2.053939295442481\n",
      "pre - post == 1.7029859204041329\n",
      "pre - post == 1.69619619846344\n",
      "pre - post == 1.6207939762818189\n",
      "pre - post == 1.775942265987396\n",
      "pre - post == 1.427805135124608\n",
      "pre - post == 1.8194445183402617\n",
      "pre - post == 1.9855187598027682\n",
      "pre - post == 2.268119172046059\n",
      "pre - post == 1.2877876444866785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre - post == 1.8804758630300822\n",
      "pre - post == 1.423560487596612\n",
      "pre - post == 1.6426235186426263\n",
      "pre - post == 1.601747613204153\n",
      "pre - post == 1.8330490714625307\n",
      "pre - post == 1.4822702784287303\n",
      "pre - post == 1.4856366044596623\n",
      "pre - post == 1.5768395976016396\n",
      "meta_test_loss: 1.500049, meta_test_acc: 0.558947\n",
      "pre - post == 1.5722664782875464\n",
      "pre - post == 1.7076808841604934\n",
      "pre - post == 1.826871671174702\n",
      "pre - post == 1.4020087530738428\n",
      "pre - post == 1.7301521803203381\n",
      "pre - post == 1.8411021828651428\n",
      "pre - post == 1.8586185982352807\n",
      "pre - post == 1.5629290342330933\n",
      "pre - post == 1.5005729449422736\n",
      "pre - post == 1.7157215507406938\n",
      "pre - post == 1.5923456016339754\n",
      "pre - post == 1.6888899928645082\n",
      "pre - post == 1.4024873030813116\n",
      "pre - post == 1.8873041554501184\n",
      "pre - post == 2.164044282938305\n",
      "pre - post == 1.9029730432911922\n",
      "pre - post == 1.593194421968962\n",
      "pre - post == 1.661699520914178\n",
      "pre - post == 1.5152206420898438\n",
      "pre - post == 2.2204294330195378\n",
      "pre - post == 1.8495164105766697\n",
      "pre - post == 1.9536004944851524\n",
      "pre - post == 1.83310119729293\n",
      "pre - post == 1.9402514790233816\n",
      "pre - post == 1.7326271220257408\n",
      "pre - post == 1.4010070185912284\n",
      "pre - post == 1.5985882658707469\n",
      "pre - post == 2.134722003811284\n",
      "pre - post == 1.806998588536915\n",
      "pre - post == 2.016775162596452\n",
      "meta_test_loss: 1.528006, meta_test_acc: 0.528684\n",
      "pre - post == 1.515367972223382\n",
      "pre - post == 1.7032530245028046\n",
      "pre - post == 1.359265270986055\n",
      "pre - post == 1.5215847805926674\n",
      "pre - post == 1.4943752477043553\n",
      "pre - post == 1.608672066738731\n",
      "pre - post == 1.2226939452321905\n",
      "pre - post == 1.473431537025853\n",
      "pre - post == 1.4904054999351501\n",
      "pre - post == 1.4559149365676078\n",
      "pre - post == 1.574236731780203\n",
      "pre - post == 1.4469888774972215\n",
      "pre - post == 1.864123413437291\n",
      "pre - post == 1.5642170560987374\n",
      "pre - post == 1.6658279331106887\n",
      "pre - post == 1.766416129313017\n",
      "pre - post == 1.348893918489155\n",
      "pre - post == 1.4223176429146214\n",
      "pre - post == 1.7158992290496826\n",
      "pre - post == 1.800994967159472\n",
      "pre - post == 1.8833727397416766\n",
      "pre - post == 1.6343446719019037\n",
      "pre - post == 1.5435058945103692\n",
      "pre - post == 1.8208540866249487\n",
      "pre - post == 1.8348790720889443\n",
      "pre - post == 2.112933889815682\n",
      "pre - post == 1.631344895613821\n",
      "pre - post == 1.7899506782230576\n",
      "pre - post == 2.287282385324177\n",
      "pre - post == 1.6381232362044487\n",
      "meta_test_loss: 1.531960, meta_test_acc: 0.530263\n",
      "pre - post == 1.7670316194233142\n",
      "pre - post == 1.965182210269727\n",
      "pre - post == 1.9357544026876752\n",
      "pre - post == 1.9982045732046427\n",
      "pre - post == 2.1031536365810193\n",
      "pre - post == 1.9797707607871606\n",
      "pre - post == 2.3994821623751994\n",
      "pre - post == 1.893821534357573\n",
      "pre - post == 2.0448320570744967\n",
      "pre - post == 1.5964852759712622\n",
      "pre - post == 1.7147338202125149\n",
      "pre - post == 2.1410892605781555\n",
      "pre - post == 2.0890160014754846\n",
      "pre - post == 1.9832290850187604\n",
      "pre - post == 1.7956709610788446\n",
      "pre - post == 1.70135277823398\n",
      "pre - post == 2.299736681737398\n",
      "pre - post == 1.7257657803987203\n",
      "pre - post == 1.3849617431038306\n",
      "pre - post == 2.1557517616372364\n",
      "pre - post == 2.303228591617785\n",
      "pre - post == 2.0229442339194446\n",
      "pre - post == 1.6581456096548781\n",
      "pre - post == 1.843403464869449\n",
      "pre - post == 1.4127032003904645\n",
      "pre - post == 1.7645344420483238\n",
      "pre - post == 1.578128921358209\n",
      "pre - post == 1.6799295638736929\n",
      "pre - post == 1.88959832254209\n",
      "pre - post == 1.5816525660063092\n",
      "meta_test_loss: 1.496765, meta_test_acc: 0.546316\n",
      "pre - post == 2.012165298587397\n",
      "pre - post == 1.8725173912550273\n",
      "pre - post == 1.513656126825433\n",
      "pre - post == 1.8353194186561987\n",
      "pre - post == 1.9829982660318675\n",
      "pre - post == 2.086203684932307\n",
      "pre - post == 1.9919717907905579\n",
      "pre - post == 1.5803609396281995\n",
      "pre - post == 1.8599300164925423\n",
      "pre - post == 1.5712785281633077\n",
      "pre - post == 2.2602368593215942\n",
      "pre - post == 1.8902400543815212\n",
      "pre - post == 2.17040836497357\n",
      "pre - post == 2.0386418229655217\n",
      "pre - post == 1.4079040916342482\n",
      "pre - post == 2.2510038112339217\n",
      "pre - post == 1.6723433105569137\n",
      "pre - post == 1.5731985569000242\n",
      "pre - post == 1.8627625327361257\n",
      "pre - post == 1.6896551031815379\n",
      "pre - post == 1.3555825572264821\n",
      "pre - post == 1.8012403908528778\n",
      "pre - post == 1.927977449015567\n",
      "pre - post == 1.4897818690852118\n",
      "pre - post == 1.3347681258854112\n",
      "pre - post == 2.24332626869804\n",
      "pre - post == 1.8103827238082888\n",
      "pre - post == 1.672788758026926\n",
      "pre - post == 1.668240321309943\n",
      "pre - post == 1.7511097130022548\n",
      "meta_test_loss: 1.522438, meta_test_acc: 0.533421\n",
      "pre - post == 2.003540951954691\n",
      "pre - post == 1.7035765396921256\n",
      "pre - post == 1.9202560500094765\n",
      "pre - post == 1.892298503925926\n",
      "pre - post == 1.6586585985986813\n",
      "pre - post == 1.8516649384247628\n",
      "pre - post == 1.7899536019877385\n",
      "pre - post == 1.3582684240843121\n",
      "pre - post == 2.3782692112420736\n",
      "pre - post == 2.147156978908338\n",
      "pre - post == 1.9048663001311454\n",
      "pre - post == 2.2320391284792045\n",
      "pre - post == 1.9256444792998466\n",
      "pre - post == 1.8835433784284092\n",
      "pre - post == 1.6955744718250476\n",
      "pre - post == 1.7978901486647756\n",
      "pre - post == 2.103545367717743\n",
      "pre - post == 2.420429866564901\n",
      "pre - post == 2.1441570551771867\n",
      "pre - post == 1.7367389578568309\n",
      "pre - post == 1.9480240658709878\n",
      "pre - post == 2.002193705031746\n",
      "pre - post == 1.6664365341788843\n",
      "pre - post == 1.519653546182733\n",
      "pre - post == 1.7245899564341494\n",
      "pre - post == 2.0042489108286405\n",
      "pre - post == 2.276410071473373\n",
      "pre - post == 2.1408452422995317\n",
      "pre - post == 1.7623789310455324\n",
      "pre - post == 2.198271130260668\n",
      "meta_test_loss: 1.508164, meta_test_acc: 0.513421\n",
      "pre - post == 1.9350921354795756\n",
      "pre - post == 1.873118701734041\n",
      "pre - post == 1.8694623645983246\n",
      "pre - post == 1.6492956625787836\n",
      "pre - post == 2.091980698861574\n",
      "pre - post == 1.851461234845613\n",
      "pre - post == 1.4778596915696798\n",
      "pre - post == 1.8783473215605084\n",
      "pre - post == 1.9910813193572197\n",
      "pre - post == 2.1123175307324056\n",
      "pre - post == 2.066506310513145\n",
      "pre - post == 1.6544525184129413\n",
      "pre - post == 1.3966098835593774\n",
      "pre - post == 2.0269632841411394\n",
      "pre - post == 2.09718687597074\n",
      "pre - post == 2.0631936600333765\n",
      "pre - post == 1.5711745902111656\n",
      "pre - post == 1.8560769118760763\n",
      "pre - post == 1.8604308304033783\n",
      "pre - post == 1.932391693717555\n",
      "pre - post == 1.8688631434189646\n",
      "pre - post == 2.2999576675264457\n",
      "pre - post == 2.094523912981937\n",
      "pre - post == 1.6715370667608163\n",
      "pre - post == 1.929204278870633\n",
      "pre - post == 2.007011215937765\n",
      "pre - post == 2.328231278218721\n",
      "pre - post == 2.2711638337687443\n",
      "pre - post == 1.940549317159151\n",
      "pre - post == 1.9848560345800297\n",
      "meta_test_loss: 1.508336, meta_test_acc: 0.535526\n",
      "pre - post == 2.0937344149539343\n",
      "pre - post == 2.065099631485186\n",
      "pre - post == 1.9406395334946485\n",
      "pre - post == 1.9869611796579862\n",
      "pre - post == 2.019790166302731\n",
      "pre - post == 1.9607711214768258\n",
      "pre - post == 1.7202302405708714\n",
      "pre - post == 1.861878689966704\n",
      "pre - post == 1.6949484536522312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-074d37d449b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmeta_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmeta_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-006ebbce6049>\u001b[0m in \u001b[0;36mmeta_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# test_lossの微分はmaster_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaster_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# ??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    143\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    144\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "meta_learner = MetaLearner()\n",
    "\n",
    "for i in range(1000):\n",
    "    meta_learner.meta_train()\n",
    "    meta_learner.meta_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
